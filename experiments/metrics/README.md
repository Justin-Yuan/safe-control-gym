# Docs

Documentation on this experiment branch (under the current folder **safe_control_gym/experiments/metrics**). 

All data is currently sync-ed to DSL drive [link](https://drive.google.com/drive/u/2/folders/1T8ZkAsBqe2Oq5vXsDO9OMVZ-431Lbgjp).



## How to generate data

Navigate to the current directory, use the following command
```
python test_metric.py --func collect_data_to_hdf5 --algo {ALGO} --task {TASK} --overrides {ENV_CONFIG_PATH} {ALGO_CONFIG_PATH} --eval_output_dir {...} --hdf5_file_name {...} --n_episodes {...} --seed {...}
```
arguments
- `--func collect_data_to_hdf5`. Data is generated by the utility function **collect_data_to_hdf5**.
- `--algo {ALGO}`. Data is generated with controller **ALGO**, options are `random, pid, lqr, mpc, ppo, ...`.
- `--task {TASK}`. Data is generated in environment **TASK**, options are `cartpole, quadrotor`.
- `--overrides {ENV_CONFIG_PATH} {ALGO_CONFIG_PATH}`. Config files **ENV_CONFIG_PATH, ALGO_CONFIG_PATH** for the task and controller respectively. 
- `--eval_output_dir {...} --hdf5_file_name {...}`. The output data is saved in HDF5 format with path **eval_output_dir/hdf5_file_name**.
- `--n_episodes`. Specifies how many trajectories to collect.
- `--seed`. Specifies the seed number that allows deterministically generating the data for reproducibility.

Example command generates data (10 trajectories) with a randomly exploring controller in the quadrotor 2D tracking task, produces data file **temp_data/data/base/quad2d_random.hdf5**.
```
python test_metric.py --func collect_data_to_hdf5 --algo random --task quadrotor --overrides config_overrides/quad_track_2d_data/base.yaml config_overrides/quad_track_2d_data/random.yaml --eval_output_dir temp_data/data --hdf5_file_name base/quad2d_random.hdf5 --n_episodes 10 --seed 10
```



## How to use the (saved) data 

Basics of using HDF5 data in python, see [here](https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/). 

In **test_metric.py**, each HDF5 data format follows the protocol (assume `f` is the HDF5 file handle)
- meta data
    - saved in top-level dictionary `f.attrs**
    - **config**, a json encoded string of the overall config when generating the data, can be reconstructed by `json.loads(f.attrs["config"])`
    - **algo**, **task**, **n_episodes**, **seed**, config terms as suggested by the name, can also be retrieved from **config** but is saved as top-level here for easier reference.
    - **restore**, **checkpoint**, optional config terms if data is generated from a trained model with specific checkpoint.
- trajectory data
    - saved in the data group **trajectory_data**, containing separate datasets by data field as key, can be accessed by, e.g. `f["trajectory_data"]["obs"], f["trajectory_data"]["action"]`. Check the `RecordDataWrapper` in **experiment.py** for all the data fields saved.
    - each data field saves the episodes as list of list of vectors (for numpy data such as obs, action) or dictionaries (for the info dict). The list of list is essentially (num_episodes x length_of_episode). To access the observations in the k-th episode, use `f["trajectory_data"]["obs"][str(k)]`. 
- additional data
    - other data groups include **controller_data**, **safety_filter_data**, can be accessed by `f["controller_data"]`, `f["safety_filter_data"]`. The exact format is not implemented currently since it depends on how you saved these data originally. 

This data protocol is implemented in utility functions `collect_data_to_hdf5()` and `save_list_defaultdict_to_hdf5()` in **test_metric.py**. You can modify them to save/load data with a new protocol.

For loading the saved HDF5 data, check out the example utility function `load_data_from_hdf5()` from **test_metric.py**, which takes in a list of HDF5 data paths and a possibly common parent data directory, and returns a dictionary combining all the data fields (along the dimension of number of episodes for each data field). 



## Notes

- The reason to choose HDF5 is that it allows loading partial data from the file, avoiding having everything loaded up in the memory which can be large. This can allow computation with the data in bachtes. 
- I tend to save a different HDF5 file for each algo+task+seed, and combine them when needed by feeding in the paths of the HDF5 files, which helps to keep track and to be modular. 



## TODOs

- integrate the HDF5 data into the metric computation via batching. 
- generate data across tasks and seeds, add exploration as well.
- setup syncing of data to the google drive?