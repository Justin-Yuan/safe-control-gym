algo_config:
  # model args
  hidden_dim: 256

  # loss args
  use_gae: True
  entropy_coef: 0.01

  # optim args
  opt_epochs: 10
  mini_batch_size: 256
  actor_lr: 0.0003
  critic_lr: 0.001

  # runner args
  max_env_steps: 200000
  rollout_batch_size: 4
  rollout_steps: 1000

  # misc
  log_interval: 1000
  save_interval: 1000
  num_checkpoints: 100
  eval_interval: 1000
  eval_save_best: True
  tensorboard: True

task_config:
  normalized_rl_action_space: True

  init_state:
    init_x: 0
    init_x_dot: 0
    # since rand is additive on this base value
    # init_z: 1
    init_z: 0
    init_z_dot: 0
    init_theta: 0
    init_theta_dot: 0
  randomized_init: True
  randomized_inertial_prop: False

  init_state_randomization_info:
    init_x:
      distrib: 'uniform'
      low: -1
      high: 1
    init_x_dot:
      distrib: 'uniform'
      low: -0.1
      high: 0.1
    init_z:
      distrib: 'uniform'
      low: 1
      high: 2
    init_z_dot:
      distrib: 'uniform'
      low: -0.1
      high: 0.1
    init_theta:
      distrib: 'uniform'
      low: -0.2
      high: 0.2
    init_theta_dot:
      distrib: 'uniform'
      low: -0.1
      high: 0.1

  cost: rl_reward
  obs_goal_horizon: 1

  # RL Reward
  rew_state_weight: [1, 0.01, 1, 0.01, 0.01, 0.01]
  rew_act_weight: 0.01
  rew_exponential: True

  done_on_out_of_bound: True
  done_on_violation: False